# 特征提取知识点

参考链接: https://www.zhihu.com/question/28641663

## 简介

一个典型的机器学习任务，是通过样本的特征来预测样本所对应的值。如果样本的特征少，我们会考虑增加特征。而现实中的情况往往是特征太多了，需要减少一些特征。

首先“无关特征”（irrelevant feature）。比如，通过空气的湿度，环境的温度，风力和当地人的男女比例来预测明天是否会下雨，其中男女比例就是典型的无关特征。

其实“多于特征”（redundant feature），比如，通过房屋的面积，卧室的面积，车库的面积，所在城市的消费水平，所在城市的税收水平等特征来预测房价，那么消费水平（或税收水平）就是多余特征。证据表明，税收水平和消费水平存在相关性，我们只需要其中一个特征就足够了，因为另一个能从其中一个推演出来。（若是线性相关，则用线性模型做回归时会出现多重共线性问题，将会导致过拟合）

减少特征具有重要的现实意义，**不仅减少过拟合、减少特征数量（降维）、提高模型泛化能力，而且还可以使模型获得更好的解释性，增强对特征和特征值之间的理解，加快模型的训练速度**，一般的，还会获得更好的性能。问题是，在面对未知领域时，很难有足够的认识去判断特征与目标之间的相关性，特征与特征之间的相关性。这时候就需要用一些数学或工程上的方法来帮助我们更好地进行特征选择。

数据挖掘

## 常见方法

对多元变量进行处理，例如特征工程中，可以使用互信息方法来选择多个对因变量有较强相关性的自变量作为特征，还可以使用主成分分析法来消除一些冗余的自变量来降低运算复杂度。

常见的方法有：

#### 过滤法（Filter）：

##### 方差选择法

##### 相关系数法

##### 卡方检验 

按照发散性或者相关性对各个特征进行评分，设定阈值 或者 设定待选择阈值的个数，选择特征

#### 包裹法（Wrapper）：
根据目标函数，每次选择若干特征或者排除若干特征，直到选择出最佳的子集。
#### **嵌入法**（Embedding）：
先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

### Filter 1.去掉取值变化小的特征

假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。

当特征值都是离散型变量(概率)的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是**不太好用**。

**可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。**

使用**方差选择法**时，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征：

```python
# 方差选择法
from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
# VarianceThreshold 移除了第一列特征，第一列中特征值为0的概率达到了5/6.
print(sel.fit_transform(X))
```

```python
[[0 1]
 [1 0]
 [0 0]
 [1 1]
 [1 0]
 [1 1]]
```



### Filter 2.单变量特征选择

**单变量特征选择的原理是分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。**

对于*分类问题(y离散)* 可采用：

**卡方检验，f_classif, mutual_info_classif，互信息**

对于*回归问题(y连续)* 可采用：

**皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数**

这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）。这种方法有许多改进的版本、变种。单变量特征选择基于单变量的统计测试来选择最佳特征。它可以看作预测模型的一项预处理。

Scikit-learn将特征选择程序用包含 transform 函数的对象来展现：

- SelectKBest 移除得分前 k 名以外的所有特征(取top k)
- SelectPercentile 移除得分在用户指定百分比以后的特征(取top k%)
- 对每个特征使用通用的单变量统计检验： 假正率(false positive rate) SelectFpr, 伪发现率(false discovery rate) SelectFdr, 或族系误差率 SelectFwe.
- GenericUnivariateSelect 可以设置不同的策略来进行单变量特征选择。同时不同的选择策略也能够使用超参数寻优，从而让我们找到最佳的单变量特征选择策略。
  　　将特征输入到评分函数，返回一个单变量的f_score(F检验的值)或p-values(P值，假设检验中的一个标准，P-value用来和显著性水平作比较)，注意SelectKBest 和 SelectPercentile只有得分，没有p-value。
- For classification: chi2, f_classif, mutual_info_classif
- For regression: f_regression, mutual_info_regression

**!! 注意：**这些基于 F-test 的方法计算两个随机变量之间的线性相关程度。另一方面，mutual information methods（互信息）能够计算任何种类的统计相关性，但是作为非参数的方法，互信息需要更多的样本来进行准确的估计。如果你使用的是稀疏的数据 (例如数据可以由稀疏矩阵来表示), chi2 , mutual_info_regression , mutual_info_classif 可以处理数据并保持它的稀疏性

### 2.1 卡方检验

经典的卡方检验chi2是检验定性自变量对定性因变量的相关性。比如，我们可以对样本进行一次c h i 2 chi^2*c**h**i*2 测试来选择最佳的两项特征：

```python
# 卡方检验
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

iris = load_iris()
x, y = iris.data, iris.target
print(x.shape)  # (150, 4)

# 卡方检验 取top k
x_new = SelectKBest(chi2, k=2).fit_transform(x, y)
print(x_new.shape)  # (150, 2)
```

```python
(150, 4)
(150, 2)
```



### 2.2 Pearson相关系数

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[ − 1 ， 1 ] -1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。

Pearson Correlation速度快、易于计算，经常在拿到数据 (经过清洗和特征提取之后的) 之后第一时间就执行。Scipy的 *pearsonr* 方法能够同时计算 *相关系数* 和*p-value*.

我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。

```python
# pearsonr(x, y)  # 计算特征与目标变量之间的相关度
# 1）输入：x为特征，y为目标变量.
# 2）输出：r： 相关系数[-1，1]之间，p - value: p值。
# 注： p值越小，表示相关系数越显著，一般p值在500个样本以上时有较高的可靠性。
# 输出为二元组(sorce, p-value)的数组
import numpy as np
from scipy.stats import pearsonr

np.random.seed(0)
size = 300
x = np.random.normal(0, 1, size)
# pearsonr(x, y)的输入为特征矩阵和目标向量
print("Lower noise", pearsonr(x, x + np.random.normal(0, 1, size)))  # 相关性较大
print("Higher noise", pearsonr(x, x + np.random.normal(0, 10, size)))  # 相关性较小

# Lower noise (0.71824836862138386, 7.3240173129992273e-49)
# Higher noise (0.057964292079338148, 0.31700993885324746)
```

**Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：**

```python
x = np.random.uniform(-1, 1, 100000)
print(pearsonr(x, x**2)[0])  # 平方关系
# 结果 -0.00230804707612
```



### 2.3 距离相关系数 (Distance Correlation)

距离相关系数是为了克服Pearson相关系数的弱点而生的。在 *x* 和 x^2 这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。

当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。

1. Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。

　2. Pearson相关系数的取值区间是[ − 1 ， 1 ]，而距离相关系数都是[ 0 ， 1 ]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。



### 2.4 基于模型的特征排序

这种方法的思路是直接使用你要用的机器学习算法，**针对 每个单独的特征 和 响应变量建立预测模型**。假如 特征 和 响应变量 之间的关系是**非线性的**，可以用基于树的方法(决策树、随机森林)、或者 扩展的线性模型 等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是**运用交叉验证**。

在 波士顿房价数据集 上使用sklearn的 随机森林回归 给出一个单变量选择的例子(这里使用了交叉验证)：

```python
# 通过随机森林回归找特征
# 要注意过拟合问题，因此树的深度最好不要太大
from sklearn.model_selection import cross_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# 加载波士顿房屋数据集
boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rf = RandomForestRegressor(n_estimators=20, max_depth=4)
scores = []
print(X[:, 1:1 + 1])
# 单独采用每个特征进行建模，并进行交叉验证
for i in range(X.shape[1]):  # 列数 13个特征
    score = cross_val_score(rf, X[:, i:i + 1], Y, scoring="r2", cv=5)
    # 注意X[:, i]shape(1,m)和X[:, i:i+1]的区别shape(m,1)
    scores.append((format(np.mean(score), '.3f'), names[i]))
print(sorted(scores, reverse=True))
```

```python
[('0.407', 'LSTAT'), ('0.172', 'RM'), ('-1.009', 'DIS'), ('-0.707', 'NOX'), ('-0.674', 'CHAS'), ('-0.629', 'ZN'), ('-0.582', 'CRIM'), ('-0.519', 'TAX'), ('-0.463', 'B'), ('-0.456', 'AGE'), ('-0.374', 'PTRATIO'), ('-0.283', 'RAD'), ('-0.143', 'INDUS')]
```



### 4.使用SelectFromModel选择特征 

单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。其实Pearson相关系数等价于线性回归里的标准化回归系数。

*SelectFromModel* 作为*meta-transformer*，能够用于拟合后任何拥有`coef_`或`feature_importances_` 属性的预测模型。 如果特征对应的`coef_`或 `feature_importances_` 值低于设定的阈值threshold，那么这些特征将被移除。除了手动设置阈值，也可通过字符串参数调用内置的启发式算法(heuristics)来设置阈值，包括：`平均值(“mean”)`, `中位数(“median”)`以及他们与浮点数的乘积，如`”0.1*mean”`。

### 4.1基于L1的特征选择

使用L1范数作为惩罚项的线性模型(Linear models)会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 `feature_selection.SelectFromModel` 来选择不为0的系数。

参数C控制稀疏性：C越小，被选中的特征越少。

对于Lasso，参数alpha越大，被选中的特征越少。

特别指出，常用于预测模型有 `linear_model.Lasso`（回归）， `linear_model.LogisticRegression` 和 `svm.LinearSVC`（分类）:

```python
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel

iris = load_iris()
X, y = iris.data, iris.target
print(X.shape)  # (150, 4)

l_svc = LinearSVC(C=0.01, penalty="l1", dual=False, max_iter=10000).fit(X, y)
model = SelectFromModel(l_svc, prefit=True)
X_new = model.transform(X)
print(X_new.shape)  # (150, 3)
```



### R

1. 选择N个特征
2. 交给xgboost 和 RandomForest 跑, 得到特征重要性
3. 若为数值预测, 用xgboost加入lambda进行变量筛选
4. 剔除特征, 重新fit

其它方法

RFE LASSO 比较耗时

目的: 规避多重共线性, 导致过拟合 欠拟合

特征之间交互性