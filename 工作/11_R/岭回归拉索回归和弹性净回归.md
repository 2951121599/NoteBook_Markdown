# 岭回归拉索回归和弹性净回归



如下五类模型的变量选择可采用R语言的glmnet包来解决。这五类模型分别是：

1. 二分类logistic回归模型
2. 多分类logistic回归模型
3. Possion模型
4. Cox比例风险模型
5. SVM

```R
参数family规定了回归模型的类型:

----family="gaussian"适用于一维连续因变量

----family=mgaussian"适用于多维连续因变量

----family="poisson"适用于非负次数因变量(count)

----family="binomial"适用于二元离散因变量(binary)

----family="multinomial"适用于多元离散因变量(category)
```



```R
cv.fit=cv.glmnet(x,y,family='binomial',type.measure="deviance")

这里的type.measure是用来指定交叉验证选取模型时希望最小化的目标参量，对与logistic回归有以下几种选择:

--------type.measure=deviance使用deviance，即-2log-likelihood（默认）

--------type.measure=mse使用拟合因变量与实际因变量的mean squred error

--------type.measure=mae使用mean absolute error

--------type.measure=class使用模型分类的错误率

--------type.measure=auc使用area under the ROC curve,是现在最流行的综合考量模型性能的一种参数

\>cv.fit$lambda.min  #最佳lambda值
```





### 参考链接

https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/

### Glmnet 包

Glmnet主要用于拟合广义线性模型。筛选可以使loss达到最小的正则化参数lambda。该算法非常快，并且可以使用稀疏矩阵作为输入。主要有线性模型用于回归，logistic回归进行分类以及cox模型进行生存分析。可以从拟合模型中做出各种预测。它也可以拟合多响应线性回归。

<img src="C:\Users\cuite\Documents\工作\截图\公式.png" style="zoom:80%;" />

其中`l(y,η)`是观测i的负对数似然，样本不同的分布具有不同的形式，对于高斯分布可以写为 1/2(y−η)^2，后一项是elastic-net正则化项，β 是需要学习的参数，α 指定使用Lasso回归（α = 1）还是岭回归（α = 0）。当我们具有较多的特征时，我们希望进行特征的筛选，Lasso回归会使特征稀疏化，保留部分特征用于构建模型，如果我们不希望舍去任何一个特征，那么便可以使用岭回归。

对于每种模型Glmnet都提供了`glmnet`用于拟合模型, `cv.glmnet`使用k折交叉验证拟合模型, `predict`对数据进行预测（分类/回归），`coef`用于提取指定lambda时特征的系数。





### 岭回归

请注意，alpha的值是Ridge的超参数，这意味着模型不会自动学习它们，而是必须手动设置它们。

在这里，我们考虑alpha = 0.05。但是，让我们考虑不同的alpha值，并为每种情况绘制系数。

### 重要事项：

- 它会缩小参数，因此主要用于防止多重共线性。
- 它通过系数收缩降低了模型的复杂性。
- 它使用L2正则化技术。（我将在本文稍后讨论）

现在让我们考虑另一种使用正则化的回归技术。



### LASSO回归

```R
from sklearn.linear_model import Lasso

lassoReg = Lasso(alpha=0.3, normalize=True)

lassoReg.fit(x_train,y_train)

pred = lassoReg.predict(x_cv)

# calculating mse

mse = np.mean((pred_cv - y_cv)**2)

mse

1346205.82

lassoReg.score(x_cv,y_cv)

0.5720
```

套索模型的预测优于线性和岭预测。

我们可以看到，随着我们增加alpha值，系数接近于零，但是如果您看到套索的情况，即使在较小的alpha值下，我们的系数也将减小为绝对零。因此，套索只选择某些特征，而将其他特征的系数减小为零。此属性称为特征选择，在山脊的情况下不存在。

### 重要事项：

- 它使用L1正则化技术（将在本文后面讨论）
- 当我们具有更多特征时，通常使用它，因为它会自动进行特征选择。

现在，您已经对ridge和lasso回归有了基本的了解，我们来看一个示例，其中有一个大型数据集，假设它具有10,000个特征。而且我们知道某些独立功能与其他独立功能相关。然后想想，您将使用Rigde还是Lasso回归？

让我们一一讨论。如果我们对它应用岭回归，它将保留所有特征，但会缩小系数。但是问题在于，由于有10,000个功能部件，模型仍将保持复杂性，因此可能会导致模型性能不佳。

如果我们对这个问题应用套索回归，那将是什么呢？套索回归的主要问题是当我们有相关变量时，它仅保留一个变量并将其他相关变量设置为零。这可能会导致某些信息丢失，从而导致模型的准确性降低。

那么该问题的解决方案是什么？实际上，我们还有另一种类型的回归，称为弹性净回归，它基本上是岭和套索回归的混合体。因此，让我们尝试了解它。

### Elastic Net Regression 弹性净回归

```R
from sklearn.linear_model import ElasticNet

ENreg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)

ENreg.fit(x_train,y_train)

pred_cv = ENreg.predict(x_cv)

#calculating mse

mse = np.mean((pred_cv - y_cv)**2)

mse 1773750.73

ENreg.score(x_cv,y_cv)

0.4504
```



现在，您已经对ridge，套索和elasticnet回归有了基本的了解。但是在此期间，我们遇到了两项L1和L2，它们基本上是两种类型的正则化。概括起来，套索和岭分别是L1和L2正则化的直接应用。

